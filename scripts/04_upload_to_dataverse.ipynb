{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import math\n",
    "import requests\n",
    "import subprocess\n",
    "\n",
    "server_url = 'https://dataverse.harvard.edu'\n",
    "api_token = ''\n",
    "dataset_id = 'doi:10.7910/DVN/Q2KIES'\n",
    "file_path = \"../data/fnc_transcripts_html_2025.tar.gz\" # fnc_transcripts_text_2025.tar.gz fnc_transcripts_text_2025.tar.gz ../data/foxnews-transcript-urls-2025.csv.gz\n",
    "upload_url = f\"{server_url}/api/datasets/:persistentId/add\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    " params = {\n",
    "        'persistentId': dataset_id,\n",
    "        'key': api_token\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Uploading fnc_transcripts_html_2025.tar.gz to dataset doi:10.7910/DVN/Q2KIES...\n",
      "Error during upload: string longer than 2147483647 bytes\n"
     ]
    }
   ],
   "source": [
    "filename = os.path.basename(file_path)\n",
    "file_obj = open(file_path, 'rb')\n",
    "files = {\n",
    "    'file': (filename, file_obj, 'application/x-gzip')\n",
    "}\n",
    "\n",
    "# Upload the file\n",
    "print(f\"Uploading {filename} to dataset {dataset_id}...\")\n",
    "try:\n",
    "    response = requests.post(\n",
    "        upload_url, \n",
    "        params=params,\n",
    "        files=files\n",
    "    )\n",
    "    \n",
    "    # Check response\n",
    "    if response.status_code == 200:\n",
    "        result = response.json()\n",
    "        print(f\"Upload successful! File ID: {result['data']['files'][0]['dataFile']['id']}\")\n",
    "        print(f\"Access the file at: {result['data']['files'][0]['dataFile']['persistentId']}\")\n",
    "    else:\n",
    "        print(f\"Upload failed with status code {response.status_code}\")\n",
    "        print(f\"Response: {response.text}\")\n",
    "except Exception as e:\n",
    "    print(f\"Error during upload: {str(e)}\")\n",
    "finally:\n",
    "    # Always close the file object\n",
    "    file_obj.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original file size: 3.56 GB\n",
      "Will split into 4 chunks of 1GB (last chunk may be smaller)\n",
      "Uploading part 1/4: fnc_transcripts_html_2025.tar.gz.part1...\n",
      "✅ Success! File ID: 11014501\n",
      "Uploading part 2/4: fnc_transcripts_html_2025.tar.gz.part2...\n",
      "✅ Success! File ID: 11014510\n",
      "Uploading part 3/4: fnc_transcripts_html_2025.tar.gz.part3...\n"
     ]
    }
   ],
   "source": [
    "chunk_size = \"1G\"  # 1GB chunks\n",
    "\n",
    "# Get original file size\n",
    "original_size = os.path.getsize(file_path)\n",
    "num_chunks = math.ceil(original_size / (1024**3))  # Calculate number of 1GB chunks needed\n",
    "print(f\"Original file size: {original_size / (1024**3):.2f} GB\")\n",
    "print(f\"Will split into {num_chunks} chunks of 1GB (last chunk may be smaller)\")\n",
    "\n",
    "# Upload URL\n",
    "upload_url = f\"{server_url}/api/datasets/:persistentId/add\"\n",
    "params = {\n",
    "    'persistentId': dataset_id,\n",
    "    'key': api_token\n",
    "}\n",
    "\n",
    "# Create and upload each chunk\n",
    "for i in range(num_chunks):\n",
    "    part_file = f\"{file_path}.part{i+1}\"\n",
    "    skip_blocks = i\n",
    "    \n",
    "    # For all chunks except the last one, use count=1 to get exactly 1GB\n",
    "    if i < num_chunks - 1:\n",
    "        cmd = [\"dd\", f\"if={file_path}\", f\"of={part_file}\", \"bs=1G\", f\"skip={skip_blocks}\", \"count=1\"]\n",
    "    else:\n",
    "        # For the last chunk, don't specify count so it takes all remaining data\n",
    "        cmd = [\"dd\", f\"if={file_path}\", f\"of={part_file}\", \"bs=1G\", f\"skip={skip_blocks}\"]\n",
    "    \n",
    "    # Execute the dd command\n",
    "    subprocess.run(cmd, stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n",
    "    \n",
    "    print(f\"Uploading part {i+1}/{num_chunks}: {os.path.basename(part_file)}...\")\n",
    "    \n",
    "    with open(part_file, 'rb') as file_obj:\n",
    "        files = {\n",
    "            'file': (os.path.basename(part_file), file_obj, 'application/octet-stream')\n",
    "        }\n",
    "        \n",
    "        try:\n",
    "            response = requests.post(\n",
    "                upload_url,\n",
    "                params=params,\n",
    "                files=files\n",
    "            )\n",
    "            \n",
    "            if response.status_code == 200:\n",
    "                result = response.json()\n",
    "                file_id = result['data']['files'][0]['dataFile']['id']\n",
    "                print(f\"✅ Success! File ID: {file_id}\")\n",
    "            else:\n",
    "                print(f\"❌ Failed with status code {response.status_code}\")\n",
    "                print(f\"Response: {response.text}\")\n",
    "        except Exception as e:\n",
    "            print(f\"❌ Error: {str(e)}\")\n",
    "\n",
    "print(\"\\nUpload completed. To reassemble the file, use:\")\n",
    "print(f\"cat {file_path}.part* > {os.path.basename(file_path)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
